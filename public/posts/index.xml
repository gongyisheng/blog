<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yisheng&#39;s Blog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Yisheng&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 May 2023 12:00:00 -0700</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>为什么我的网络传输速度下降了</title>
      <link>http://localhost:1313/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E9%80%9F%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BA%86/</link>
      <pubDate>Sat, 06 May 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E9%80%9F%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BA%86/</guid>
      <description>&lt;h3 id=&#34;背景&#34;&gt;背景&lt;/h3&gt;&#xA;&lt;p&gt;这个问题一开始是在进行RDS实验的时候发现的。最初的情景是，多台机器同时对数据库进行select和insert操作时，会发现insert操作会造成select操作qps大幅下降，且insert操作结束之后select操作的qps仍不能回升。起初以为是RDS的问题，但是在复现问题、监控RDS之后发现RDS的压力其实很小。于是开始怀疑是网络的问题，在简化了场景和操作之后，发现能在过去做tcp实验的机器上复现，于是用这个更简单的场景进行问题复现和分析。&lt;/p&gt;&#xA;&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;&#xA;&lt;p&gt;linux kernal version: 6.1&lt;br&gt;&#xA;linux image: amazon/al2023-ami-2023.0.20230419.0-kernel-6.1-x86_64&lt;br&gt;&#xA;instance type: AWS t2.micro (1 vCPU, 1GiB RAM)&lt;br&gt;&#xA;tc qdisc: fq_codel&lt;/p&gt;&#xA;&lt;p&gt;无网络流量情况下：&lt;br&gt;&#xA;network rtt: 0.5 ms&lt;br&gt;&#xA;network bandwidth: 60 MB/s&lt;/p&gt;&#xA;&lt;h3 id=&#34;操作&#34;&gt;操作&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;开三台 t2.micro 机器，一台做 server，两台做client，在server上放一个 2GB 大小的文件&lt;/li&gt;&#xA;&lt;li&gt;client1 通过 curl get server 文件&lt;/li&gt;&#xA;&lt;li&gt;等待一段时间（约20s），client1网速稳定后，client2 通过 curl get server 文件&lt;/li&gt;&#xA;&lt;li&gt;可以观察到两种情况（如果无法复现请多试几次，就能复现）&lt;br&gt;&#xA;正常情况：两个 client 速度稳定在30MB/s左右&lt;br&gt;&#xA;降速情况：两个 client 速度降低到3.5MB/s左右&lt;/li&gt;&#xA;&lt;li&gt;关掉client2，观察到client1恢复到7-8MB/s，但是远低于60MB/s的带宽上限&lt;/li&gt;&#xA;&lt;li&gt;降速情况发生之后，client1/2重新通过curl 下载 server文件，就会出现起初网络速度在30-50MB/s，但10s后会降速7-8MB/s的情况，需要重启server才能恢复到60MB/s&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;分析&#34;&gt;分析&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;抓包&lt;/p&gt;&#xA;&lt;p&gt;通过tcpdump抓包，获得了完整的降速过程中，server和两个client的网络包的传输情况，pcap文件已上传到&lt;a href=&#34;https://github.com/gongyisheng/playground/tree/main/network/throughput_drop_case&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/rtt_server_to_client1_serverside.png&#34; alt=&#34;img&#34;&gt;&#xA;&lt;img src=&#34;./images/rtt_server_to_client1_clientside.png&#34; alt=&#34;img&#34;&gt;&#xA;&lt;img src=&#34;./images/rtt_server_to_client2_serverside.png&#34; alt=&#34;img&#34;&gt;&#xA;&lt;img src=&#34;./images/rtt_server_to_client2_clientside.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;可以观察到：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;rtt有明显上升，rtt从降速前的小于1ms上升到降速后的50-100ms&lt;/li&gt;&#xA;&lt;li&gt;两边rtt不对等，在降速后关掉client2, server-&amp;gt;client1 tcp stream的rtt在server端和client端差异很大&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;猜测A：bufferbloat&lt;/p&gt;</description>
    </item>
    <item>
      <title>Packet capture experiment 2</title>
      <link>http://localhost:1313/posts/packet_capture_experiment_2/</link>
      <pubDate>Sat, 15 Apr 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/packet_capture_experiment_2/</guid>
      <description>&lt;h2 id=&#34;sendreceive-buffer&#34;&gt;send/receive buffer&lt;/h2&gt;&#xA;&lt;h3 id=&#34;goal&#34;&gt;Goal&lt;/h3&gt;&#xA;&lt;p&gt;Get a better understanding of how send/receive buffer size may affect the network rtt and throughput. Usually send/receive buffer size is managed by kernal and application should not hardcode these values. However, real world cases are more rare and complicated. Inapporiate buffer size may be the root case of some network issues. Rare but possible.&lt;/p&gt;&#xA;&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Instance&lt;/p&gt;&#xA;&lt;p&gt;Instance provider: AWS&lt;br&gt;&#xA;Instance type: t2.micro, 1vCPU, 1GiB memory, Low to Moderate network&lt;br&gt;&#xA;Region: us-east-1b&lt;br&gt;&#xA;OS: Amazon Linux 2023 AMI&lt;br&gt;&#xA;Python: 3.9&lt;/p&gt;</description>
    </item>
    <item>
      <title>Packet capture experiment 1</title>
      <link>http://localhost:1313/posts/packet_capture_experiment_1/</link>
      <pubDate>Tue, 11 Apr 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/packet_capture_experiment_1/</guid>
      <description>&lt;h2 id=&#34;packet-delay-loss-duplicate-corrupt-out-of-order-and-bandwidth-limit&#34;&gt;packet delay, loss, duplicate, corrupt, out-of-order and bandwidth limit&lt;/h2&gt;&#xA;&lt;h3 id=&#34;goal&#34;&gt;Goal&lt;/h3&gt;&#xA;&lt;p&gt;The goal of packet capture experiment here is to get a better understanding of TCP protocol. Besides, by reproducing the common network issues in the real world like packet delay, loss, out-of-order and brandwidth limit and saving a snapshot of packet capture result of circumstances above, we can identify and solve these problems more quickly next time.&lt;/p&gt;&#xA;&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Instance&lt;/p&gt;&#xA;&lt;p&gt;Instance provider: AWS&lt;br&gt;&#xA;Instance type: t2.micro, 1vCPU, 1GiB memory, Low to Moderate network&lt;br&gt;&#xA;Region: us-east-1b&lt;br&gt;&#xA;OS: Amazon Linux 2023 AMI&lt;br&gt;&#xA;Python: 3.9&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spot termination makes runtime unstable</title>
      <link>http://localhost:1313/posts/spot_termination_makes_runtime_unstable/</link>
      <pubDate>Sat, 18 Mar 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/spot_termination_makes_runtime_unstable/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;p&gt;Spark Job runtime is not stable&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/spot-instance-1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;As what can see from the screenshot, the pipeline (spark job) runtime is not quite stable. However, the pipeline deals with almost fixed amount data everyday, the runtime should not be so quite unstable like this.&lt;/p&gt;&#xA;&lt;p&gt;Besides, the pipeline runtime is quite stable at around 15min before migration to databricks.&lt;/p&gt;&#xA;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spark driver log&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;23/03/16 19:04:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230316185728-0000/2 is now EXITED (Worker shutting down)&#xA;23/03/16 19:04:26 WARN DLTDebugger: Failed to talk to RPC endpoint: dlt-debugger&#xA;org.apache.spark.SparkException: Exception thrown in awaitResult: &#xA;    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)&#xA;    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)&#xA;    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)&#xA;    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)&#xA;    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:46)&#xA;    at org.apache.spark.debugger.DLTDebugger$.liftedTree1$1(DLTDebugger.scala:258)&#xA;    at org.apache.spark.debugger.DLTDebugger$.getDebuggerRef(DLTDebugger.scala:257)&#xA;    at org.apache.spark.debugger.DLTDebugger$.report(DLTDebugger.scala:341)&#xA;    at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.$anonfun$applyOrElse$5(StandaloneAppClient.scala:188)&#xA;    at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)&#xA;    at scala.Option.foreach(Option.scala:407)&#xA;    at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:184)&#xA;    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)&#xA;    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)&#xA;    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)&#xA;    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)&#xA;    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:110)&#xA;    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)&#xA;    at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:74)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:60)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:107)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:110)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:750)&#xA;Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://dlt-debugger@10.241.97.77:45081&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:175)&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:171)&#xA;    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)&#xA;    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)&#xA;    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)&#xA;    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:105)&#xA;    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)&#xA;    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)&#xA;    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)&#xA;    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)&#xA;    at scala.concurrent.Future.flatMap(Future.scala:306)&#xA;    at scala.concurrent.Future.flatMap$(Future.scala:306)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:177)&#xA;    ... 25 more&#xA;23/03/16 19:04:26 INFO StandaloneSchedulerBackend: Executor app-20230316185728-0000/2 removed: Worker shutting down&#xA;23/03/16 19:04:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230316185728-0000/4 on worker-20230316185728-10.241.99.108-43045 (10.241.99.108:43045) with 4 core(s)&#xA;23/03/16 19:04:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20230316185728-0000/4 on hostPort 10.241.99.108:43045 with 4 core(s), 12.0 GiB RAM&#xA;23/03/16 19:04:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230316185728-0000/4 is now FAILED (java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.)&#xA;23/03/16 19:04:26 INFO StandaloneSchedulerBackend: Executor app-20230316185728-0000/4 removed: java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spark log4j-active log shows that there’s excutor exit. But no reason for that.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Broadcast variable cause OOM</title>
      <link>http://localhost:1313/posts/broadcast_variable_case_oom/</link>
      <pubDate>Wed, 15 Mar 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/broadcast_variable_case_oom/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spark cluster master node OOM&lt;/p&gt;&#xA;&lt;p&gt;Master node shutdown due to OOM. Error log:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;INFO Data stored in hdfs:///XXXX&#xA;INFO XXXXX updated&#xA;INFO Data has XXXXX records&#xA;INFO Data stored in hdfs:///XXXX&#xA;INFO XXXXX updated&#xA;#&#xA;# java.lang.OutOfMemoryError: Java heap space&#xA;# -XX:OnOutOfMemoryError=&amp;#34;kill -9 %p&amp;#34;&#xA;#   Executing /bin/sh -c &amp;#34;kill -9 *****&amp;#34;...&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spark cluster worker node shutdown&lt;/p&gt;&#xA;&lt;p&gt;Worker nodes worked well until driver shutdown. Error log:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ERROR YarnCoarseGrainedExecutorBackend: Executor self-exiting due to : Driver ip-***-***-***-***.ec2.internal:***** disassociated! Shutting down.&#xA;INFO MemoryStore: MemoryStore cleared&#xA;INFO BlockManager: BlockManager stopped&#xA;ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Look up driver log&lt;/p&gt;</description>
    </item>
    <item>
      <title>Redis OOM due to big keys</title>
      <link>http://localhost:1313/posts/redis_oom_due_to_big_keys/</link>
      <pubDate>Thu, 16 Feb 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/redis_oom_due_to_big_keys/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Queue client performance went down&lt;/p&gt;&#xA;&lt;p&gt;Everything worked well in the morning until we got monitoring alarm at 12:45 PM EST: &lt;code&gt;&amp;lt;topic_name&amp;gt; Queue lag is too big&lt;/code&gt;. The issue happened in a service which is responsible for consuming message from message queue, processing message and writing it to database. The service also uses Redis to cache some objects which can be reused every time it processes the message. Auto-scaling rule is applied to the service so there’ll be tens to hundreds of pods running under heavy workload. We observed that the consumer speed went down by 50%, causing messages to be backlogged in the queue.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MySQL connection deadlock</title>
      <link>http://localhost:1313/posts/mysql_connection_deadlock/</link>
      <pubDate>Fri, 10 Feb 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/mysql_connection_deadlock/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;h4 id=&#34;cronjob-is-taking-more-than-1h-to-complete&#34;&gt;CronJob is taking more than 1h to complete&lt;/h4&gt;&#xA;&lt;p&gt;My colleagues told me that one of the cron job stuck in the middle after a random day. They received the warning: CronJob is taking more than 1h to complete. And the pod kept stucking there after a day, which is abnormal. However, another cron job which almost uses the same code works well. No database failure was reported during the period of time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why signal file is a bad idea</title>
      <link>http://localhost:1313/posts/why_signal_file_is_a_bad_idea/</link>
      <pubDate>Fri, 13 Jan 2023 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/why_signal_file_is_a_bad_idea/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;p&gt;Signal file is widely used in Hadoop ecosystem. If you have experience with MapReduce, you’ll notice that by default MapReduce runtime writes an empty _SUCCESS file to mark successful completion of a job to the output folder. AWS DataPipeline and Databricks also support “file arrival” to trigger a downstream job.&lt;/p&gt;&#xA;&lt;h3 id=&#34;question&#34;&gt;Question&lt;/h3&gt;&#xA;&lt;p&gt;Is signal file a good architecture design?&lt;/p&gt;&#xA;&lt;p&gt;Can I use _SUCCESS created by MapReduce as signal file to trigger downstream job?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
