<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="Observation


Queue client performance went down
Everything worked well in the morning until we got monitoring alarm at 12:45 PM EST: &lt;topic_name&gt; Queue lag is too big. The issue happened in a service which is responsible for consuming message from message queue, processing message and writing it to database. The service also uses Redis to cache some objects which can be reused every time it processes the message. Auto-scaling rule is applied to the service so there’ll be tens to hundreds of pods running under heavy workload. We observed that the consumer speed went down by 50%, causing messages to be backlogged in the queue.">  

  <title>
    
      Redis OOM due to big keys
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.51652302d3a998bf7887aed5c2cf89141bbebdf45a2c8f87b0717a3cf4f51c4e53c694c328fb1de78c3a625a1c01f80745bf1f2f42c040647a245cbbb6c2d1d7.css" integrity="sha512-UWUjAtOpmL94h67Vws&#43;JFBu&#43;vfRaLI&#43;HsHF6PPT1HE5TxpTDKPsd54w6YlocAfgHRb8fL0LAQGR6JFy7tsLR1w==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2023-02-16 12:00:00 -0700 -0700">
            2023-02-16
        </time>
    </p>

    <h1>Redis OOM due to big keys</h1>

    

    <h3 id="observation">Observation</h3>
<ol>
<li>
<p>Queue client performance went down<br>
Everything worked well in the morning until we got monitoring alarm at 12:45 PM EST: <code>&lt;topic_name&gt; Queue lag is too big</code>. The issue happened in a service which is responsible for consuming message from message queue, processing message and writing it to database. The service also uses Redis to cache some objects which can be reused every time it processes the message. Auto-scaling rule is applied to the service so there’ll be tens to hundreds of pods running under heavy workload. We observed that the consumer speed went down by 50%, causing messages to be backlogged in the queue.</p>
</li>
<li>
<p>Redis memory went up for X10 times, finally OOM<br>
Next, we receive another alarm: <code>&lt;Redis_name&gt; Redis memory usage increase over 30% per hour</code> at 1:00 PM EST. Actually the monitoring dashboard showed that Redis memory went up by X10 times! It raised from 100M to 1GB. It started at 12:39 PM, which is before queue lag is too big warning. The memory usage is stable at 1G to 1.5G at first, but went up gradually and finally OOM 3hrs later.
<img src="../assets/images/redis_oom_due_to_big_keys/redis-oom-1.jpg" alt="img"></p>
</li>
<li>
<p>Client side error: Timeout reading from socket<br>
I looked up the log of the service and found this error: <code>Timeout reading from socket</code>. We have Redis socketTimeout = 300s, which means that the client doesn’t get message back from Redis after 5min!
<img src="../assets/images/redis_oom_due_to_big_keys/redis-oom-2.png" alt="img"></p>
</li>
<li>
<p>Network out speed was limited at 300 Mbps<br>
The network out speed is very stable and limited at 300 Mbps.Before 12:39 PM, the network out speed fluctuated between 300-500 Mbps.
<img src="../assets/images/redis_oom_due_to_big_keys/redis-oom-3.jpg" alt="img"></p>
</li>
</ol>
<h3 id="analysis">Analysis</h3>
<ol>
<li>
<p>Which line of code throw out the error<br>
Almost every line of code to get key from redis throws this error, It’s not caused by a specific line of code</p>
</li>
<li>
<p>redis-cli, SLOWLOG GET<br>
There’s timeout error, so it worths to take a look at slowlog. I saw there were some keys that took about 10s to load, but it happened long before 12:39 PM EST. Nothing suspecious was found.</p>
</li>
<li>
<p>redis-cli, MEMORY STATS<br>
Since I can’t get any information to slove the problem from performance downgrade and Timeout error, I have to switch to another clue to investigate: Redis memory increase. I use MEMORY STATS to see which causes memory usage to increase.</p>
<pre tabindex="0"><code>MEMORY STATS
1) &#34;peak.allocated&#34;
2) (integer) 3761107656
3) &#34;total.allocated&#34;
4) (integer) 1171592296
5) &#34;startup.allocated&#34;
6) (integer) 1417976
7) &#34;replication.backlog&#34;
8) (integer) 0
9) &#34;clients.slaves&#34;
10) (integer) 0
11) &#34;clients.normal&#34;
12) (integer) 996545640
13) &#34;aof.buffer&#34;
14) (integer) 0
15) &#34;lua.caches&#34;
16) (integer) 0
17) &#34;db.0&#34;
18) 1) &#34;overhead.hashtable.main&#34;
    2) (integer) 954544
    3) &#34;overhead.hashtable.expires&#34;
    4) (integer) 677536
19) &#34;overhead.total&#34;
20) (integer) 999595696
21) &#34;keys.count&#34;
22) (integer) 17310
23) &#34;keys.bytes-per-key&#34;
24) (integer) 67601
25) &#34;dataset.bytes&#34;
26) (integer) 171996600
27) &#34;dataset.percentage&#34;
28) &#34;14.69837474822998&#34;
29) &#34;peak.percentage&#34;
30) &#34;31.150192260742188&#34;
31) &#34;allocator.allocated&#34;
32) (integer) 1165686536
33) &#34;allocator.active&#34;
34) (integer) 1174016000
35) &#34;allocator.resident&#34;
36) (integer) 1226031104
37) &#34;allocator-fragmentation.ratio&#34;
38) &#34;1.0071455240249634&#34;
39) &#34;allocator-fragmentation.bytes&#34;
40) (integer) 8329464
41) &#34;allocator-rss.ratio&#34;
42) &#34;1.0443053245544434&#34;
43) &#34;allocator-rss.bytes&#34;
44) (integer) 52015104
45) &#34;rss-overhead.ratio&#34;
46) &#34;0.93485987186431885&#34;
47) &#34;rss-overhead.bytes&#34;
48) (integer) -79863808
49) &#34;fragmentation&#34;
50) &#34;0.98332488536834717&#34;
51) &#34;fragmentation.bytes&#34;
52) (integer) -19436600
</code></pre><p>The result showed that clients.normal is too big (about 1G), dataset.percentage is only 14%. The problem is not caused by a burst of newly added data, but something with Redis client overhead.</p>
</li>
<li>
<p>redis-cli, CLIENT LIST<br>
Okay, we know that there’s problems with redis client overhead. So which client overhead cause the problem? I use CLIENT LIST to get the information and statistics about the client connections. The result showed that there were too many clients with tot-mem=7M/5M, oll=1.</p>
<pre tabindex="0"><code>id=2136697 addr=***.***.***.***:***** laddr=***.***.***.***:***** fd=508 name= age=3034 idle=2 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 argv-mem=0 obl=0 oll=0 omem=0 tot-mem=20504 events=r cmd=get user=default redir=-1
id=2135439 addr=***.***.***.***:***** laddr=***.***.***.***:***** fd=766 name= age=6148 idle=1 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=40954 argv-mem=0 obl=0 oll=1 omem=7340056 tot-mem=7401512 events=rw cmd=get user=default redir=-1
id=2135638 addr=***.***.***.***:***** laddr=***.***.***.***:***** fd=589 name= age=5784 idle=1 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 argv-mem=0 obl=0 oll=1 omem=5242904 tot-mem=5263408 events=rw cmd=get user=default redir=-1
</code></pre><p>It looked messages were backloged in the out buffer to be sent out. And we know that the network out speed is limited at 300 MB, is there anything limits the network speed?</p>
</li>
<li>
<p>Work with DevOps team<br>
Confirmed that no speed limit rule set by DevOps team.
<img src="../assets/images/redis_oom_due_to_big_keys/redis-oom-4.png" alt="img">
Confirmed that the network speed of instance Redis pod running in hit the limit of AWS EC2 instance we use around 12:39 PM EST by DevOps team<br>
<img src="../assets/images/redis_oom_due_to_big_keys/redis-oom-5.png" alt="img"></p>
</li>
</ol>
<h3 id="root-cause">Root Cause</h3>
<ol>
<li>
<p>Redis big key<br>
There’re several big keys stored in Redis. Each one is about 5 – 7Mb. Imagine if there’re 100 pods request the same key in a second, it requires 500-700 Mbps network brandwith to work well.</p>
</li>
<li>
<p>Then hit network speed limit of AWS instance we use.<br>
Network output of redis pod is limited at 300 Mbps when the instance network hit the upperbound (4750 Mbps).</p>
</li>
<li>
<p>Data to be sent out is backlogged in the buffer, causing memory to increase, finally OOM</p>
<p>The data to be sent out line up in the network output buffer, which can be read from clients.normal and takes more and more memory, finally OOM.</p>
</li>
</ol>
<h3 id="solution">Solution</h3>
<p>Avoid requesting Redis big key. Since this key is not frequently updated and has a loose consistency requirement, we use memory cache instead.</p>
<h3 id="learned">Learned</h3>
<ol>
<li>
<p>Avoid Redis big key.<br>
Don’t store big keys in Redis which may be frequently requested! If you have to store big key in Redis, avoid frequent request for big key and use MGET to improve the performance. Loading big key frequently can cause a lot of problems (Redis cpu usage too high, get command takes too much time, and hits network speed limit)</p>
</li>
<li>
<p>Everything has a limit.<br>
Estimate the qps and network I/O that your service will bring to Redis (and other external services) before use them.</p>
</li>
</ol>

</article>

                
    
    
        A blog since 2023 | Made by <a href="https://gohugo.io/">hugo</a> | Host on <a href="https://www.cloudflare.com/">cloudflare</a>
    


            </div>
        </main>
    </body>
</html>
