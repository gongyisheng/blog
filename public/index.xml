<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yisheng&#39;s Blog</title>
    <link>https://blog.yellowday.day/</link>
    <description>Recent content on Yisheng&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Feb 2023 12:00:00 -0700</lastBuildDate>
    <atom:link href="https://blog.yellowday.day/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Redis OOM due to big keys</title>
      <link>https://blog.yellowday.day/redis_oom_due_to_big_keys/</link>
      <pubDate>Thu, 16 Feb 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/redis_oom_due_to_big_keys/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Queue client performance went down&lt;br&gt;&#xA;Everything worked well in the morning until we got monitoring alarm at 12:45 PM EST: &lt;code&gt;&amp;lt;topic_name&amp;gt; Queue lag is too big&lt;/code&gt;. The issue happened in a service which is responsible for consuming message from message queue, processing message and writing it to database. The service also uses Redis to cache some objects which can be reused every time it processes the message. Auto-scaling rule is applied to the service so there’ll be tens to hundreds of pods running under heavy workload. We observed that the consumer speed went down by 50%, causing messages to be backlogged in the queue.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MySQL connection deadlock</title>
      <link>https://blog.yellowday.day/mysql_connection_deadlock/</link>
      <pubDate>Fri, 10 Feb 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/mysql_connection_deadlock/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;h4 id=&#34;cronjob-is-taking-more-than-1h-to-complete&#34;&gt;CronJob is taking more than 1h to complete&lt;/h4&gt;&#xA;&lt;p&gt;My colleagues told me that one of the cron job stuck in the middle after a random day. They received the warning: CronJob is taking more than 1h to complete. And the pod kept stucking there after a day, which is abnormal. However, another cron job which almost uses the same code works well. No database failure was reported during the period of time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why signal file is a bad idea</title>
      <link>https://blog.yellowday.day/why_signal_file_is_a_bad_idea/</link>
      <pubDate>Fri, 13 Jan 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/why_signal_file_is_a_bad_idea/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;p&gt;Signal file is widely used in Hadoop ecosystem. If you have experience with MapReduce, you’ll notice that by default MapReduce runtime writes an empty _SUCCESS file to mark successful completion of a job to the output folder. AWS DataPipeline and Databricks also support “file arrival” to trigger a downstream job.&lt;/p&gt;&#xA;&lt;h3 id=&#34;question&#34;&gt;Question&lt;/h3&gt;&#xA;&lt;p&gt;Is signal file a good architecture design?&lt;/p&gt;&#xA;&lt;p&gt;Can I use _SUCCESS created by MapReduce as signal file to trigger downstream job?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
