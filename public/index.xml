<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yisheng&#39;s Blog</title>
    <link>https://blog.yellowday.day/</link>
    <description>Recent content on Yisheng&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Apr 2023 12:00:00 -0700</lastBuildDate>
    <atom:link href="https://blog.yellowday.day/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Packet capture experiment 1</title>
      <link>https://blog.yellowday.day/packet_capture_experiment_1/</link>
      <pubDate>Tue, 11 Apr 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/packet_capture_experiment_1/</guid>
      <description>&lt;h2 id=&#34;packet-delay-loss-duplicate-corrupt-out-of-order-and-bandwidth-limit&#34;&gt;packet delay, loss, duplicate, corrupt, out-of-order and bandwidth limit&lt;/h2&gt;&#xA;&lt;h3 id=&#34;goal&#34;&gt;Goal&lt;/h3&gt;&#xA;&lt;p&gt;The goal of packet capture experiment here is to get a better understanding of TCP protocol. Besides, by reproducing the common network issues in the real world like packet delay, loss, out-of-order and brandwidth limit and saving a snapshot of packet capture result of circumstances above, we can identify and solve these problems more quickly next time.&lt;/p&gt;&#xA;&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Instance&lt;/p&gt;&#xA;&lt;p&gt;Instance provider: AWS&lt;br&gt;&#xA;Instance type: t2.micro, 1vCPU, 1GiB memory, Low to Moderate network&lt;br&gt;&#xA;Region: us-east-1b&lt;br&gt;&#xA;OS: Amazon Linux 2023 AMI&lt;br&gt;&#xA;Python: 3.9&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spot termination makes runtime unstable</title>
      <link>https://blog.yellowday.day/spot_termination_makes_runtime_unstable/</link>
      <pubDate>Sat, 18 Mar 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/spot_termination_makes_runtime_unstable/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;p&gt;Spark Job runtime is not stable&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/spot-instance-1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;As what can see from the screenshot, the pipeline (spark job) runtime is not quite stable. However, the pipeline deals with almost fixed amount data everyday, the runtime should not be so quite unstable like this.&lt;/p&gt;&#xA;&lt;p&gt;Besides, the pipeline runtime is quite stable at around 15min before migration to databricks.&lt;/p&gt;&#xA;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spark driver log&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;23/03/16 19:04:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230316185728-0000/2 is now EXITED (Worker shutting down)&#xA;23/03/16 19:04:26 WARN DLTDebugger: Failed to talk to RPC endpoint: dlt-debugger&#xA;org.apache.spark.SparkException: Exception thrown in awaitResult: &#xA;    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)&#xA;    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)&#xA;    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)&#xA;    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)&#xA;    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:46)&#xA;    at org.apache.spark.debugger.DLTDebugger$.liftedTree1$1(DLTDebugger.scala:258)&#xA;    at org.apache.spark.debugger.DLTDebugger$.getDebuggerRef(DLTDebugger.scala:257)&#xA;    at org.apache.spark.debugger.DLTDebugger$.report(DLTDebugger.scala:341)&#xA;    at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.$anonfun$applyOrElse$5(StandaloneAppClient.scala:188)&#xA;    at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)&#xA;    at scala.Option.foreach(Option.scala:407)&#xA;    at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:184)&#xA;    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)&#xA;    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)&#xA;    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)&#xA;    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)&#xA;    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:110)&#xA;    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)&#xA;    at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:74)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:60)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:107)&#xA;    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:110)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:750)&#xA;Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://dlt-debugger@10.241.97.77:45081&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:175)&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:171)&#xA;    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)&#xA;    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)&#xA;    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)&#xA;    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:105)&#xA;    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)&#xA;    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)&#xA;    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)&#xA;    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)&#xA;    at scala.concurrent.Future.flatMap(Future.scala:306)&#xA;    at scala.concurrent.Future.flatMap$(Future.scala:306)&#xA;    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:177)&#xA;    ... 25 more&#xA;23/03/16 19:04:26 INFO StandaloneSchedulerBackend: Executor app-20230316185728-0000/2 removed: Worker shutting down&#xA;23/03/16 19:04:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230316185728-0000/4 on worker-20230316185728-10.241.99.108-43045 (10.241.99.108:43045) with 4 core(s)&#xA;23/03/16 19:04:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20230316185728-0000/4 on hostPort 10.241.99.108:43045 with 4 core(s), 12.0 GiB RAM&#xA;23/03/16 19:04:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230316185728-0000/4 is now FAILED (java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.)&#xA;23/03/16 19:04:26 INFO StandaloneSchedulerBackend: Executor app-20230316185728-0000/4 removed: java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spark log4j-active log shows that there’s excutor exit. But no reason for that.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Broadcast variable cause OOM</title>
      <link>https://blog.yellowday.day/broadcast_variable_case_oom/</link>
      <pubDate>Wed, 15 Mar 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/broadcast_variable_case_oom/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spark cluster master node OOM&lt;/p&gt;&#xA;&lt;p&gt;Master node shutdown due to OOM. Error log:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;INFO Data stored in hdfs:///XXXX&#xA;INFO XXXXX updated&#xA;INFO Data has XXXXX records&#xA;INFO Data stored in hdfs:///XXXX&#xA;INFO XXXXX updated&#xA;#&#xA;# java.lang.OutOfMemoryError: Java heap space&#xA;# -XX:OnOutOfMemoryError=&amp;#34;kill -9 %p&amp;#34;&#xA;#   Executing /bin/sh -c &amp;#34;kill -9 *****&amp;#34;...&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spark cluster worker node shutdown&lt;/p&gt;&#xA;&lt;p&gt;Worker nodes worked well until driver shutdown. Error log:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ERROR YarnCoarseGrainedExecutorBackend: Executor self-exiting due to : Driver ip-***-***-***-***.ec2.internal:***** disassociated! Shutting down.&#xA;INFO MemoryStore: MemoryStore cleared&#xA;INFO BlockManager: BlockManager stopped&#xA;ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Look up driver log&lt;/p&gt;</description>
    </item>
    <item>
      <title>Redis OOM due to big keys</title>
      <link>https://blog.yellowday.day/redis_oom_due_to_big_keys/</link>
      <pubDate>Thu, 16 Feb 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/redis_oom_due_to_big_keys/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Queue client performance went down&lt;/p&gt;&#xA;&lt;p&gt;Everything worked well in the morning until we got monitoring alarm at 12:45 PM EST: &lt;code&gt;&amp;lt;topic_name&amp;gt; Queue lag is too big&lt;/code&gt;. The issue happened in a service which is responsible for consuming message from message queue, processing message and writing it to database. The service also uses Redis to cache some objects which can be reused every time it processes the message. Auto-scaling rule is applied to the service so there’ll be tens to hundreds of pods running under heavy workload. We observed that the consumer speed went down by 50%, causing messages to be backlogged in the queue.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MySQL connection deadlock</title>
      <link>https://blog.yellowday.day/mysql_connection_deadlock/</link>
      <pubDate>Fri, 10 Feb 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/mysql_connection_deadlock/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;h4 id=&#34;cronjob-is-taking-more-than-1h-to-complete&#34;&gt;CronJob is taking more than 1h to complete&lt;/h4&gt;&#xA;&lt;p&gt;My colleagues told me that one of the cron job stuck in the middle after a random day. They received the warning: CronJob is taking more than 1h to complete. And the pod kept stucking there after a day, which is abnormal. However, another cron job which almost uses the same code works well. No database failure was reported during the period of time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why signal file is a bad idea</title>
      <link>https://blog.yellowday.day/why_signal_file_is_a_bad_idea/</link>
      <pubDate>Fri, 13 Jan 2023 12:00:00 -0700</pubDate>
      <guid>https://blog.yellowday.day/why_signal_file_is_a_bad_idea/</guid>
      <description>&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;&#xA;&lt;p&gt;Signal file is widely used in Hadoop ecosystem. If you have experience with MapReduce, you’ll notice that by default MapReduce runtime writes an empty _SUCCESS file to mark successful completion of a job to the output folder. AWS DataPipeline and Databricks also support “file arrival” to trigger a downstream job.&lt;/p&gt;&#xA;&lt;h3 id=&#34;question&#34;&gt;Question&lt;/h3&gt;&#xA;&lt;p&gt;Is signal file a good architecture design?&lt;/p&gt;&#xA;&lt;p&gt;Can I use _SUCCESS created by MapReduce as signal file to trigger downstream job?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
